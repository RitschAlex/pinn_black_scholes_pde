{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0347fc95-ef1f-4ad8-a250-8ce0aaec15d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae03a15-0278-41e2-aece-cb8244b39380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch._dynamo\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._functorch.config.donated_buffer=False\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.dpi': 100,\n",
    "    'legend.loc': 'upper left',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f76d1b-0bac-425c-bc89-a921b4aaa023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50_000\n",
    "LEARNING_RATE = 4e-4\n",
    "DATA_BATCH_SIZE = int(4_096*1.5)\n",
    "COLLOC_BATCH_SIZE = int(DATA_BATCH_SIZE*3)\n",
    "WARMUP_EPOCHS = 1_500\n",
    "RAMPUP_EPOCHS = 5_000\n",
    "PDE_WEIGHT_TARGET = 0.51\n",
    "LEARNING_RATE_FACTOR = 0.5\n",
    "LEARNING_RATE_PATIENCE = 750\n",
    "R_M = 0.60\n",
    "R_T = 0.35\n",
    "ATM_TC_SAMPLING = 0.20\n",
    "ATM_COLLOC_SAMPLING = 0.25\n",
    "T_SKEW=1.40\n",
    "M_SKEW=1.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f3994-5227-4d32-af94-ab1a6672dd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'filtered_data.csv')\n",
    "call_df = df[df['Option_Type'] == 'put'].copy()\n",
    "call_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "call_df.loc[:, 'Dividend_Yield'] = call_df['Dividend_Yield'] / 100\n",
    "call_df.loc[:, 'RF'] = call_df['RF'] / 100\n",
    "call_df.loc[:, 'Implied_Volatility'] = call_df['Implied_Volatility'] / 100\n",
    "\n",
    "max_tau = 90/365.25\n",
    "call_df.loc[:, 't'] = max_tau - call_df['Time_to_Exp']\n",
    "call_df = call_df[call_df['Time_to_Exp'] <= max_tau]\n",
    "\n",
    "moneyness_labels = ['OTM', 'ATM', 'ITM']\n",
    "moneyness_conditions = [\n",
    "    call_df['Moneyness'] > 1.03,\n",
    "    (call_df['Moneyness'] >= 0.97) & (call_df['Moneyness'] <= 1.03),\n",
    "    call_df['Moneyness'] < 0.97\n",
    "]\n",
    "call_df['moneyness_labels'] = np.select(moneyness_conditions, moneyness_labels, default='Other')\n",
    "\n",
    "X_market = call_df[['Underlying_Price', 'Strike', 't', 'RF', 'Dividend_Yield']].to_numpy()\n",
    "Y_market = call_df[['Option_Price']].to_numpy()\n",
    "market_moneyness_labels = call_df['moneyness_labels'].to_numpy()\n",
    "\n",
    "spot_bounds = (call_df['Underlying_Price'].min(), call_df['Underlying_Price'].max())\n",
    "strike_bounds = (call_df['Strike'].min(), call_df['Strike'].max())\n",
    "time_bounds = (call_df['t'].min(), call_df['t'].max())\n",
    "rate_bounds = (call_df['RF'].min(), call_df['RF'].max())\n",
    "dividend_bounds = (call_df['Dividend_Yield'].min(), call_df['Dividend_Yield'].max())\n",
    "moneyness_bounds = (call_df['Moneyness'].min(), call_df['Moneyness'].max())\n",
    "moneyness_mean = call_df['Moneyness'].mean()\n",
    "\n",
    "print(f\"{call_df['moneyness_labels'].value_counts(normalize=True)}\\n\")\n",
    "\n",
    "print(f\"95% Implied Volatility: {call_df['Implied_Volatility'].quantile(0.95)}\")\n",
    "print(f\"99% Implied Volatility: {call_df['Implied_Volatility'].quantile(0.99)}\")\n",
    "print(f\"99.5% Implied Volatility: {call_df['Implied_Volatility'].quantile(0.995)}\")\n",
    "print(f\"99.9% Implied Volatility: {call_df['Implied_Volatility'].quantile(0.999)}\\n\")\n",
    "\n",
    "call_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cfd2b-863c-412e-a8a2-55b6c6430dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(call_df['moneyness_labels'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc575e-b84c-4c5c-963f-ea8bf4c158b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd34a7-a1be-48c2-853f-e323d3cdab79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_collocation_points(n_samples, spot_bounds, strike_bounds,\n",
    "                                tau_bounds, rate_bounds, dividend_bounds,\n",
    "                                moneyness_bounds, moneyness_mean, seed=SEED):\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    n_atm = int(n_samples * ATM_COLLOC_SAMPLING)\n",
    "    atm_moneyness_bounds = (0.97, 1.03)\n",
    "\n",
    "    if n_atm > 0:\n",
    "        s_atm = rng.uniform(*spot_bounds, size=n_atm)\n",
    "        m_atm = rng.uniform(*atm_moneyness_bounds, size=n_atm)\n",
    "        k_atm = s_atm / m_atm\n",
    "\n",
    "        valid_mask = (k_atm >= strike_bounds[0]) * (k_atm <= strike_bounds[1])\n",
    "        s_atm, k_atm = s_atm[valid_mask], k_atm[valid_mask]\n",
    "\n",
    "        n_atm = len(s_atm)\n",
    "\n",
    "        tau_atm = rng.uniform(*tau_bounds, size=n_atm)\n",
    "        t_atm = ((90/365.25) - tau_atm)\n",
    "        r_atm = rng.uniform(*rate_bounds, size=n_atm)\n",
    "        d_atm = rng.uniform(*dividend_bounds, size=n_atm)\n",
    "\n",
    "        X_atm = np.stack([s_atm, k_atm, t_atm, r_atm, d_atm], axis=1)\n",
    "    else:\n",
    "        X_atm = np.array([]).reshape(0, 5)\n",
    "\n",
    "    n_regular = n_samples - n_atm\n",
    "    X_regular = np.empty((n_regular, 5))\n",
    "    filled = 0\n",
    "\n",
    "    batch_size = min(50_000, max(1_000, 2 * n_regular))\n",
    "\n",
    "    while filled < n_regular:\n",
    "\n",
    "        s_points = rng.uniform(*spot_bounds, size=(batch_size))\n",
    "\n",
    "        u_m = rng.uniform(0, 1, size=(batch_size))\n",
    "        m_points = moneyness_bounds[0] + (moneyness_bounds[1] - moneyness_bounds[0]) * (1 - (1 - u_m)**M_SKEW)\n",
    "\n",
    "        k_points = s_points/m_points\n",
    "\n",
    "        valid_mask = (\n",
    "            (m_points >= moneyness_bounds[0]) &\n",
    "            (m_points <= moneyness_bounds[1]) &\n",
    "            (k_points >= strike_bounds[0]) &\n",
    "            (k_points <= strike_bounds[1])\n",
    "        )\n",
    "\n",
    "        s = s_points[valid_mask]\n",
    "        k = k_points[valid_mask]\n",
    "\n",
    "        num_valid = len(s)\n",
    "        if num_valid == 0:\n",
    "            continue\n",
    "\n",
    "        u = rng.uniform(0, 1, size=(batch_size))\n",
    "        tau = tau_bounds[0] + (tau_bounds[1] - tau_bounds[0]) * (u**T_SKEW)\n",
    "        t = ((90/365.25) - tau)\n",
    "        r = rng.uniform(*rate_bounds, size=(batch_size))\n",
    "        d = rng.uniform(*dividend_bounds, size=(batch_size))\n",
    "\n",
    "        take = min(num_valid, n_regular - filled)\n",
    "\n",
    "        X_regular[filled:filled+take, 0] = s[:take]\n",
    "        X_regular[filled:filled+take, 1] = k[:take]\n",
    "        X_regular[filled:filled+take, 2] = t[:take]\n",
    "        X_regular[filled:filled+take, 3] = r[:take]\n",
    "        X_regular[filled:filled+take, 4] = d[:take]\n",
    "\n",
    "        filled += take\n",
    "\n",
    "    X = np.vstack([X_regular, X_atm])\n",
    "    rng.shuffle(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_tc_points(n_samples, atm_sampling, spot_bounds, strike_bounds, tau_bounds, rate_bounds, dividend_bounds, seed=SEED):\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    if atm_sampling > 0:\n",
    "        n_atm = n_samples\n",
    "        atm_moneyness_bounds = (0.97, 1.03)\n",
    "\n",
    "        s_atm = rng.uniform(*spot_bounds, size=n_atm)\n",
    "        m_atm = rng.uniform(*atm_moneyness_bounds, size=n_atm)\n",
    "        k_atm = s_atm / m_atm\n",
    "\n",
    "        valid_mask = (k_atm >= strike_bounds[0]) * (k_atm <= strike_bounds[1])\n",
    "        s_atm, k_atm = s_atm[valid_mask], k_atm[valid_mask]\n",
    "\n",
    "        s = np.vstack([s_atm.reshape(-1, 1)])\n",
    "        k = np.vstack([k_atm.reshape(-1, 1)])\n",
    "\n",
    "        l = np.full(len(s), \"terminal_atm\", dtype=\"<U7\")\n",
    "\n",
    "    else:\n",
    "        n_regular = n_samples\n",
    "        s_regular = rng.uniform(*spot_bounds, size=(n_regular, 1))\n",
    "        k_regular = rng.uniform(*strike_bounds, size=(n_regular, 1))\n",
    "\n",
    "        s = np.vstack([s_regular.reshape(-1, 1)])\n",
    "        k = np.vstack([k_regular.reshape(-1, 1)])\n",
    "\n",
    "        l = np.full(len(s), \"terminal_regular\", dtype=\"<U7\")\n",
    "\n",
    "    current_n_samples = len(s)\n",
    "    t = np.full((current_n_samples, 1), (90/365.25))\n",
    "    r = rng.uniform(*rate_bounds, size=(current_n_samples, 1))\n",
    "    q = rng.uniform(*dividend_bounds, size=(current_n_samples, 1))\n",
    "\n",
    "    X_tc = np.hstack([s, k, t, r, q])\n",
    "    Y_tc = np.maximum(k - s, 0.0)\n",
    "    terminal_labels = l\n",
    "\n",
    "    return X_tc, Y_tc, terminal_labels\n",
    "\n",
    "\n",
    "def generate_bc_points(n_samples, spot_bounds, strike_bounds, tau_bounds, rate_bounds, dividend_bounds, seed=SEED):\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    n_s0 = n_samples // 2\n",
    "\n",
    "    S = np.zeros((n_s0, 1))\n",
    "    K = rng.uniform(*strike_bounds, size=(n_s0, 1))\n",
    "    tau = rng.uniform(*tau_bounds, size=(n_s0, 1))\n",
    "    t = ((90/365.25) - tau)\n",
    "    r = rng.uniform(*rate_bounds, size=(n_s0, 1))\n",
    "    q = rng.uniform(*dividend_bounds, size=(n_s0, 1))\n",
    "    X_s0 = np.hstack([S, K, t, r, q])\n",
    "    Y_s0 = K * np.exp(-r * tau)\n",
    "\n",
    "    n_sinf = n_samples - n_s0\n",
    "    large_spot_val = strike_bounds[1] * 4\n",
    "\n",
    "    S = np.full((n_sinf, 1), large_spot_val)\n",
    "    K = rng.uniform(*strike_bounds, size=(n_sinf, 1))\n",
    "    tau = rng.uniform(*tau_bounds, size=(n_sinf, 1))\n",
    "    t = ((90/365.25) - tau)\n",
    "    r = rng.uniform(*rate_bounds, size=(n_sinf, 1))\n",
    "    q = rng.uniform(*dividend_bounds, size=(n_sinf, 1))\n",
    "    X_smax = np.hstack([S, K, t, r, q])\n",
    "    Y_smax = np.zeros((n_sinf, 1))\n",
    "\n",
    "    b_s0_labels = np.full(len(X_s0), \"null\", dtype=\"<U7\")\n",
    "    b_max_labels = np.full(len(X_smax), \"max\", dtype=\"<U7\")\n",
    "\n",
    "    return X_s0, X_smax, Y_s0, Y_smax, b_s0_labels, b_max_labels\n",
    "\n",
    "\n",
    "def create_train_test_split(market_data, terminal_boundary_data, train_ratio=0.8, val_ratio=0.1, seed=SEED):\n",
    "\n",
    "    test_ratio = 1.0 - train_ratio - val_ratio\n",
    "    train_sets, val_sets, test_sets = {}, {}, {}\n",
    "\n",
    "\n",
    "    X_market, Y_market, market_labels = market_data\n",
    "    X_m_train, X_m_rem, Y_m_train, Y_m_rem, labels_m_train, labels_m_rem = train_test_split(\n",
    "        X_market, Y_market, market_labels,\n",
    "        train_size=train_ratio, stratify=market_labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    relative_test_size = test_ratio / (val_ratio + test_ratio)\n",
    "    X_m_val, X_m_test, Y_m_val, Y_m_test = train_test_split(\n",
    "        X_m_rem, Y_m_rem,\n",
    "        test_size=relative_test_size, stratify=labels_m_rem, random_state=seed\n",
    "    )\n",
    "\n",
    "    train_sets['market'] = (X_m_train, Y_m_train)\n",
    "    val_sets['market'] = (X_m_val, Y_m_val)\n",
    "    test_sets['market'] = (X_m_test, Y_m_test)\n",
    "\n",
    "    for name, (X, Y, L) in terminal_boundary_data.items():\n",
    "        X_train, X_rem, Y_train, Y_rem, labels_t_b_train, labels_t_b_rem = train_test_split(\n",
    "            X, Y, L, train_size=train_ratio, stratify=L, random_state=seed\n",
    "        )\n",
    "\n",
    "        X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "            X_rem, Y_rem,\n",
    "            test_size=relative_test_size, stratify=labels_t_b_rem, random_state=seed\n",
    "        )\n",
    "\n",
    "        train_sets[name] = (X_train, Y_train)\n",
    "        val_sets[name] = (X_val, Y_val)\n",
    "        test_sets[name] = (X_test, Y_test)\n",
    "\n",
    "    all_X_val = np.vstack([arr[0] for arr in val_sets.values()])\n",
    "    all_Y_val = np.vstack([arr[1] for arr in val_sets.values()])\n",
    "\n",
    "    all_X_test = np.vstack([arr[0] for arr in test_sets.values()])\n",
    "    all_Y_test = np.vstack([arr[1] for arr in test_sets.values()])\n",
    "\n",
    "    all_X_train = np.vstack([arr[0] for arr in train_sets.values()])\n",
    "    all_Y_train = np.vstack([arr[1] for arr in train_sets.values()])\n",
    "\n",
    "    x_min = all_X_train.min(axis=0)\n",
    "    x_max = all_X_train.max(axis=0)\n",
    "    x_range = x_max - x_min\n",
    "\n",
    "    y_min = all_Y_train.min(axis=0)\n",
    "    y_max = all_Y_train.max(axis=0)\n",
    "    y_range = y_max - y_min\n",
    "\n",
    "    return train_sets, (all_X_val, all_Y_val), (all_X_test, all_Y_test), x_min, x_range, y_min, y_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0b092-e9f0-43cc-8f98-3ac52996403e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e8e55-ca8c-450b-9d69-004ee4ca2ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetricsLogger:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self._batch_metrics = {}\n",
    "\n",
    "    def log_batch(self, metrics):\n",
    "        for key, value in metrics.items():\n",
    "            if key not in self._batch_metrics:\n",
    "                self._batch_metrics[key] = []\n",
    "            self._batch_metrics[key].append(value)\n",
    "\n",
    "    def log_epoch(self, epoch_metrics):\n",
    "        epoch_summary = epoch_metrics.copy()\n",
    "        for key, values in self._batch_metrics.items():\n",
    "            epoch_summary[f'avg_{key}'] = sum(values) / len(values)\n",
    "\n",
    "        self.history.append(epoch_summary)\n",
    "        self._batch_metrics = {}\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        return pd.DataFrame(self.history)\n",
    "\n",
    "    def save_to_csv(self, filename=\"training_metrics.csv\"):\n",
    "        df = self.to_dataframe()\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Metrics saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb45fea-1e8a-4f2c-970a-014ced7c2ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        mods = []\n",
    "        for i in range(len(layers) - 2):\n",
    "            mods.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "            mods.append(nn.Tanh())\n",
    "        mods.append(nn.Linear(layers[-2], layers[-1]))\n",
    "        self.net = nn.Sequential(*mods).to(DEVICE)\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers, y_min, y_range, x_min, x_range):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = MLP(layers)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        self.register_buffer(\"y_min\",   torch.tensor(y_min,   dtype=torch.float32))\n",
    "        self.register_buffer(\"y_range\", torch.tensor(y_range, dtype=torch.float32))\n",
    "        self.register_buffer(\"x_min\",   torch.tensor(x_min,   dtype=torch.float32))\n",
    "        self.register_buffer(\"x_range\", torch.tensor(x_range, dtype=torch.float32))\n",
    "\n",
    "        self.register_buffer(\"sigma_min\", torch.tensor(0.0292, dtype=torch.float32))\n",
    "        self.register_buffer(\"sigma_max\", torch.tensor(1.7756, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x_norm):\n",
    "\n",
    "        output = self.net(x_norm)\n",
    "        pred_norm_V, sigma = output.split(1, dim=1)\n",
    "\n",
    "        V = self.softplus(pred_norm_V) * self.y_range + self.y_min\n",
    "        sigma = self.sigma_min + (self.sigma_max - self.sigma_min) * self.softplus(sigma)\n",
    "\n",
    "        return pred_norm_V, V, sigma\n",
    "\n",
    "    def pde_res(self, Xc, Xc_norm):\n",
    "\n",
    "        _, V, sigma = self.forward(Xc_norm)\n",
    "\n",
    "        grads = torch.autograd.grad(V, Xc, torch.ones_like(V), create_graph=True, retain_graph=True)\n",
    "        dV_dS = grads[0][:, 0:1]\n",
    "        dV_dt = grads[0][:, 2:3]\n",
    "\n",
    "        hess = torch.autograd.grad(dV_dS, Xc, torch.ones_like(dV_dS), create_graph=True, retain_graph=True)\n",
    "        d2V_dS2 = hess[0][:, 0:1]\n",
    "\n",
    "        S = Xc[:, 0:1]\n",
    "        r = Xc[:, 3:4]\n",
    "        q = Xc[:, 4:5]\n",
    "\n",
    "        res = (dV_dt + 0.5 * sigma**2 * S**2 * d2V_dS2 + (r - q) * S * dV_dS - r * V)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7979f29-0084-405b-8e3c-a43568659333",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505bd6c-0dcc-48fc-b79c-0917d41d24fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_grad_norm_ratio(model, X_b, Y_b, Xc, pde_w):\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    X_b_norm = 2 * (X_b - model.x_min) / model.x_range - 1\n",
    "    _, V, _ = model(X_b_norm)\n",
    "\n",
    "    loss_data = torch.mean((V - Y_b)**2)\n",
    "\n",
    "    grads_data = torch.autograd.grad(loss_data, model.parameters(), retain_graph=True)\n",
    "    grad_norm_data = torch.sqrt(sum(torch.sum(g**2) for g in grads_data))\n",
    "\n",
    "    Xc_norm = 2 * (Xc - model.x_min) / model.x_range - 1\n",
    "    pde_res = model.pde_res(Xc, Xc_norm)\n",
    "\n",
    "    loss_pde = torch.mean(pde_res**2)\n",
    "    w_loss_pde = loss_pde * pde_w\n",
    "\n",
    "    grads_pde = torch.autograd.grad(w_loss_pde, model.parameters(), retain_graph=False)\n",
    "    grad_norm_pde = torch.sqrt(sum(torch.sum(g**2) for g in grads_pde))\n",
    "\n",
    "    ratio = (grad_norm_pde / grad_norm_data).item()\n",
    "\n",
    "    return ratio, grad_norm_data.detach().item(), grad_norm_pde.detach().item()\n",
    "\n",
    "def make_train_step(model, optimizer):\n",
    "    def train_step(Xm_b, Ym_b, Xt_b, Yt_b, Xb_b, Yb_b, Xc, pde_w):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        X_b = torch.cat([Xm_b, Xt_b, Xb_b], dim=0)\n",
    "        Y_b = torch.cat([Ym_b, Yt_b, Yb_b], dim=0)\n",
    "\n",
    "        X_b_norm = 2 * (X_b - model.x_min) / model.x_range - 1\n",
    "        _, V, _ = model(X_b_norm)\n",
    "\n",
    "        n_m = Xm_b.shape[0]\n",
    "        n_t = Xt_b.shape[0]\n",
    "        n_b = Xb_b.shape[0]\n",
    "\n",
    "        V_mkt, V_tc, V_bc = torch.split(V, [n_m, n_t, n_b])\n",
    "        Y_mkt, Y_tc, Y_bc = torch.split(Y_b, [n_m, n_t, n_b])\n",
    "\n",
    "        loss_mkt = torch.mean((V_mkt - Y_mkt)**2)\n",
    "        loss_tc = torch.mean((V_tc - Y_tc)**2)\n",
    "        loss_bc = torch.mean((V_bc - Y_bc)**2)\n",
    "\n",
    "        loss_data = loss_mkt + loss_tc + loss_bc\n",
    "\n",
    "        if pde_w > 0 and Xc.shape[0] > 0:\n",
    "            Xc_norm = 2 * (Xc - model.x_min) / model.x_range - 1\n",
    "            pde_res = model.pde_res(Xc, Xc_norm)\n",
    "            loss_pde = torch.mean(pde_res**2)\n",
    "        else:\n",
    "            loss_pde = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "        w_loss_pde = loss_pde * pde_w\n",
    "        total_loss = loss_data + w_loss_pde\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss_mkt, loss_tc, loss_bc, loss_data, loss_pde\n",
    "    return train_step\n",
    "\n",
    "def train_model(model, train_sets, Xva, Yva, Xc_pool, logger):\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    train_step = make_train_step(model, optimizer)\n",
    "\n",
    "    Xm_np, Ym_np = train_sets[\"market\"]\n",
    "    Xt_np, Yt_np = train_sets[\"terminal\"]\n",
    "    Xb_np = np.vstack([train_sets[\"boundary_s0\"][0], train_sets[\"boundary_smax\"][0]]) \n",
    "    Yb_np = np.vstack([train_sets[\"boundary_s0\"][1], train_sets[\"boundary_smax\"][1]])\n",
    "\n",
    "    Xm, Ym = torch.from_numpy(Xm_np).float().to(DEVICE), torch.from_numpy(Ym_np).float().to(DEVICE)\n",
    "    Xt, Yt = torch.from_numpy(Xt_np).float().to(DEVICE), torch.from_numpy(Yt_np).float().to(DEVICE)\n",
    "    Xb, Yb = torch.from_numpy(Xb_np).float().to(DEVICE), torch.from_numpy(Yb_np).float().to(DEVICE)\n",
    "\n",
    "    X_val = torch.from_numpy(Xva).float().to(DEVICE)\n",
    "    Y_val = torch.from_numpy(Yva).float().to(DEVICE)\n",
    "    Xc_pool = torch.from_numpy(Xc_pool).float().to(DEVICE)\n",
    "\n",
    "    del train_sets, Xm_np, Ym_np, Xt_np, Yt_np, Xb_np, Yb_np\n",
    "\n",
    "    r_m, r_t = R_M, R_T\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=LEARNING_RATE_FACTOR, patience=LEARNING_RATE_PATIENCE, min_lr=1e-7)\n",
    "\n",
    "    n_m = int(DATA_BATCH_SIZE * r_m)\n",
    "    n_t = int(DATA_BATCH_SIZE * r_t)\n",
    "    n_b = DATA_BATCH_SIZE - n_m - n_t\n",
    "\n",
    "    grad_ratio = 0.0\n",
    "    grad_norm_data = 0.0\n",
    "    grad_norm_pde = 0.0\n",
    "    \n",
    "    t_start = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        e_start = time.time()\n",
    "\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            pde_w = 0.0\n",
    "            num_batches = math.ceil(Xm.shape[0] / (DATA_BATCH_SIZE))\n",
    "            n_c = 0\n",
    "        else:\n",
    "            prog = min(1.0, (epoch - WARMUP_EPOCHS) / RAMPUP_EPOCHS)\n",
    "            pde_w = PDE_WEIGHT_TARGET * prog\n",
    "            num_batches = math.ceil(Xc_pool.shape[0] / COLLOC_BATCH_SIZE)\n",
    "            n_c = COLLOC_BATCH_SIZE\n",
    "\n",
    "        sum_d = torch.tensor(0.0, device=DEVICE)\n",
    "        sum_p = torch.tensor(0.0, device=DEVICE)\n",
    "        sum_m = torch.tensor(0.0, device=DEVICE)\n",
    "        sum_t = torch.tensor(0.0, device=DEVICE)\n",
    "        sum_b = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "        colloc_idx_perm = torch.randperm(Xc_pool.shape[0], device=DEVICE)\n",
    "        market_idx_perm = torch.randperm(len(Xm), device=DEVICE)\n",
    "        bc_idx_perm = torch.randperm(len(Xb), device=DEVICE)\n",
    "        tc_idx_perm = torch.randperm(len(Xt), device=DEVICE)\n",
    "        m_ptr, t_ptr, b_ptr = 0, 0, 0\n",
    "\n",
    "        for b in range(num_batches):\n",
    "\n",
    "            m_end = m_ptr + n_m\n",
    "            m_indicies = market_idx_perm[m_ptr:m_end]\n",
    "            Xm_b, Ym_b = Xm[m_indicies], Ym[m_indicies]\n",
    "            m_ptr = m_end % len(Xm)\n",
    "\n",
    "            t_end = t_ptr + n_t\n",
    "            t_indicies = tc_idx_perm[t_ptr:t_end]\n",
    "            Xt_b, Yt_b = Xt[t_indicies], Yt[t_indicies]\n",
    "            t_ptr = t_end % len(Xt)\n",
    "\n",
    "            b_end = b_ptr + n_b\n",
    "            b_indicies = bc_idx_perm[b_ptr:b_end]\n",
    "            Xb_b, Yb_b = Xb[b_indicies], Yb[b_indicies]\n",
    "            b_ptr = b_end % len(Xb)\n",
    "\n",
    "            if n_c > 0:\n",
    "                c_start = b * n_c\n",
    "                c_end = c_start + n_c\n",
    "                c_indicies = colloc_idx_perm[c_start:c_end]\n",
    "                Xc = Xc_pool[c_indicies]\n",
    "                Xc.requires_grad_(True)\n",
    "            else:\n",
    "                Xc = torch.empty(0, Xm.shape[1], device=DEVICE, requires_grad=False)\n",
    "\n",
    "            m_l, t_l, b_l, d_l, p_l = train_step(Xm_b, Ym_b, Xt_b, Yt_b, Xb_b, Yb_b, Xc, pde_w)\n",
    "\n",
    "            sum_d += d_l.detach()\n",
    "            sum_p += p_l.detach()\n",
    "            sum_m += m_l.detach()\n",
    "            sum_t += t_l.detach()\n",
    "            sum_b += b_l.detach()\n",
    "\n",
    "        if epoch % 50 == 0 and pde_w > 0.0:\n",
    "            X_b = torch.cat([Xm_b, Xt_b, Xb_b], dim=0)\n",
    "            Y_b = torch.cat([Ym_b, Yt_b, Yb_b], dim=0)\n",
    "            grad_ratio, grad_norm_data, grad_norm_pde = get_grad_norm_ratio(model, X_b, Y_b, Xc, pde_w)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            X_val_norm = 2 * (X_val - model.x_min) / model.x_range - 1\n",
    "            _, pred_test_unnorm, sigma = model(X_val_norm)\n",
    "            mse_test = torch.mean((pred_test_unnorm - Y_val)**2).item()\n",
    "            mae_test = torch.mean(torch.abs(pred_test_unnorm - Y_val)).item()\n",
    "            model.train()\n",
    "\n",
    "        scheduler.step(mse_test)\n",
    "        \n",
    "        logger.log_epoch({\n",
    "            'epoch': epoch,\n",
    "            'market_loss': m_l.detach().item(),\n",
    "            'tc_loss': t_l.detach().item(),\n",
    "            'bc_loss': b_l.detach().item(),\n",
    "            'pde_loss': p_l.detach().item(),\n",
    "            'pde_weight': pde_w,\n",
    "            'mse_val': mse_test,\n",
    "            'mae_val': mae_test,\n",
    "            'learning_rate': scheduler.get_last_lr()[0],\n",
    "            'mean_sigma': sigma.mean().item(),\n",
    "            'max_sigma': sigma.max().item(),\n",
    "            'min_sigma': sigma.min().item(),\n",
    "            'grad_ratio': grad_ratio,\n",
    "            'grad_norm_data': grad_norm_data,\n",
    "            'grad_norm_pde': grad_norm_pde,\n",
    "        })\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            e_end = time.time() - e_start\n",
    "            avg_d_l = (sum_d / num_batches).item()\n",
    "            avg_p_l = (sum_p / num_batches).item()\n",
    "            avg_m_l = (sum_m / num_batches).item()\n",
    "            avg_t_l = (sum_t / num_batches).item()\n",
    "            avg_b_l = (sum_b / num_batches).item()\n",
    "\n",
    "            print(f\"Epoch {epoch:3d}: Data_Loss={avg_d_l:.3e}, Market_Loss={avg_m_l:.3e},\"\n",
    "                    f\" IC_Loss={avg_t_l:.3e},\"\n",
    "                    f\" BC_Loss={avg_b_l:.3e},\"\n",
    "                    f\" PDE_Loss={avg_p_l:.3e},\"\n",
    "                    f\" Lambda_PDE={pde_w:.2e},\"\n",
    "                    f\" wPDE_Loss={avg_p_l*pde_w:.3e},\"\n",
    "                    f\" Test_MSE={mse_test:.3e},\"\n",
    "                    f\" Test_MAE={mae_test:.3e},\"\n",
    "                    f\" Grad_Ratio={grad_ratio:.3e},\"\n",
    "                    f\" Learning_Rate={scheduler.get_last_lr()[0]},\"\n",
    "                    f\" Mean Sigma={sigma.mean().item():.4f},\"\n",
    "                    f\" Max Sigma={sigma.max().item():.4f},\"\n",
    "                    f\" Min Sigma={sigma.min().item():.4f},\"\n",
    "                    f\" Time Epoch={e_end:.2f}s\\n\")\n",
    "\n",
    "    t_end = time.time() - t_start\n",
    "    torch.save(net.state_dict(), f\"net_put_tanh_6_148_{EPOCHS}.pth\")\n",
    "    logger.save_to_csv()\n",
    "    \n",
    "    print(f\"--- Training Time: {t_end:.1f} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5b7e7-4d8f-4324-af99-2cf9ecb50fd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a637cb1-4d7e-4fd5-9273-b3c99f67c913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = int(len(X_market))\n",
    "\n",
    "spot_bounds = spot_bounds\n",
    "strike_bounds = strike_bounds\n",
    "tau_bounds = (0, 90/365.25)\n",
    "rate_bounds = rate_bounds\n",
    "dividend_bounds = dividend_bounds\n",
    "moneyness_bounds = moneyness_bounds\n",
    "\n",
    "moneyness_mean = moneyness_mean\n",
    "\n",
    "\n",
    "n_colloc = int(n_samples * 2.0)\n",
    "Xc_pool = generate_collocation_points(n_colloc, spot_bounds, strike_bounds, time_bounds, rate_bounds, dividend_bounds, moneyness_bounds, moneyness_mean)\n",
    "\n",
    "\n",
    "n_tc = int(n_samples * (3.0 - ATM_TC_SAMPLING))\n",
    "n_tc_atm = int(n_samples * (ATM_TC_SAMPLING + 0.007))\n",
    "diff = int(n_samples*3.0) - n_tc\n",
    "\n",
    "X_t, Y_t, t_regular_labels = generate_tc_points(n_tc, 0.0, spot_bounds, strike_bounds, time_bounds, rate_bounds, dividend_bounds)\n",
    "X_t_atm, Y_t_atm, t_atm_labels = generate_tc_points(n_tc_atm, ATM_TC_SAMPLING, spot_bounds, strike_bounds, time_bounds, rate_bounds, dividend_bounds)\n",
    "X_t_atm, Y_t_atm, t_atm_labels = X_t_atm[:diff], Y_t_atm[:diff], t_atm_labels[:diff]\n",
    "\n",
    "\n",
    "n_bc = int(n_samples * 3.0)\n",
    "X_b_s0, X_b_smax, Y_b_s0, Y_b_smax, b_s0_labels, b_smax_labels = generate_bc_points(n_bc, spot_bounds, strike_bounds, time_bounds, rate_bounds, dividend_bounds)\n",
    "\n",
    "\n",
    "terminal_boundary_data = {\"terminal\": (X_t, Y_t, t_regular_labels),\n",
    "                          \"terminal_atm\": (X_t_atm, Y_t_atm, t_atm_labels),\n",
    "                          \"boundary_smax\": (X_b_smax, Y_b_smax, b_smax_labels),\n",
    "                          \"boundary_s0\": (X_b_s0, Y_b_s0, b_s0_labels)}\n",
    "\n",
    "market_data = (X_market, Y_market, market_moneyness_labels)\n",
    "\n",
    "train_sets, val_sets, test_sets, x_min, x_range, y_min, y_range = create_train_test_split(\n",
    "    market_data, terminal_boundary_data,\n",
    "    train_ratio=0.8, val_ratio=0.1, seed=SEED\n",
    ")\n",
    "\n",
    "X_val, Y_val = val_sets\n",
    "X_test, Y_test = test_sets\n",
    "\n",
    "#del X_ic, Y_ic, X_bc, Y_bc, datasets\n",
    "\n",
    "logger = MetricsLogger()\n",
    "#net = PINN([5, 148, 148, 148, 148, 148, 148, 2], y_min, y_range, x_min, x_range).to(DEVICE)\n",
    "#net = torch.compile(net, mode=\"default\")\n",
    "\n",
    "print(f\"\\nEpochs: {EPOCHS}\")\n",
    "print(f\"Warm Up Epochs: {WARMUP_EPOCHS}\")\n",
    "print(f\"PDE Ramp Up Epochs: {RAMPUP_EPOCHS}\")\n",
    "print(f\"Data Batch Size: {DATA_BATCH_SIZE}\")\n",
    "print(f\"Collocation Batch Size: {COLLOC_BATCH_SIZE}\")\n",
    "print(f\"Number of Collocation Batches: {math.ceil(Xc_pool.shape[0]/COLLOC_BATCH_SIZE)}\")\n",
    "print(f\"Number of Data Batches: {math.ceil(len(train_sets['market'][0])/DATA_BATCH_SIZE)}\")\n",
    "print(f\"PDE Weight Target: {PDE_WEIGHT_TARGET}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE:.2e}\")\n",
    "print(f\"Learning Rate Factor: {LEARNING_RATE_FACTOR}\")\n",
    "print(f\"Learning Rate Patience: {LEARNING_RATE_PATIENCE}\")\n",
    "print(f\"Samples Collocation Points: {n_colloc}\")\n",
    "print(f\"Samples Market Data: {len(train_sets['market'][0])}\")\n",
    "print(f\"Samples IC Points: {len(train_sets['terminal'][0]) + len(train_sets['terminal_atm'][0])}\")\n",
    "print(f\"Samples BC Points: {len(train_sets['boundary_smax'][0])}\")\n",
    "print(f\"Samples BC Points: {len(train_sets['boundary_s0'][0])}\")\n",
    "print(f\"Ratio Batch Size Market Points: {R_M}\")\n",
    "print(f\"Ratio Batch Size IC Points: {R_T}\")\n",
    "print(f\"Ratio Batch Size BC Points: {1 - R_M - R_T:.2f}\")\n",
    "print(f\"ATM Ratio IC Sampling: {ATM_TC_SAMPLING}\")\n",
    "print(f\"ATM Ratio Collocation Sampling: {ATM_COLLOC_SAMPLING}\\n\")\n",
    "\n",
    "print(f\"spot_bounds: {spot_bounds}\")\n",
    "print(f\"strike_bounds: {strike_bounds}\")\n",
    "print(f\"tau_bounds: {tau_bounds}\")\n",
    "print(f\"rate_bounds: {rate_bounds}\")\n",
    "print(f\"dividend_bounds: {dividend_bounds}\")\n",
    "print(f\"moneyness_bounds: {moneyness_bounds}\")\n",
    "print(f\"moneyness_mean: {moneyness_mean}\\n\")\n",
    "print(f\"T SKEW: {T_SKEW}\")\n",
    "print(f\"M SKEW: {M_SKEW}\\n\")\n",
    "\n",
    "#train_model(net, train_sets, X_val, Y_val, Xc_pool, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e24c5-fae4-432c-a419-3d3024210ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Collocation Points Sampling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b36b4-fd97-40e0-8387-5bbff9a9e359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = Xc_pool[:, 0]\n",
    "K = Xc_pool[:, 1]\n",
    "t = Xc_pool[:, 2]\n",
    "\n",
    "moneyness = S / K\n",
    "\n",
    "time_to_expiry_years = (90 / 365.25) - t\n",
    "\n",
    "n_total_points = Xc_pool.shape[0]\n",
    "n_plot_points = 9_000\n",
    "plot_indices = np.random.choice(n_total_points, n_plot_points, replace=False)\n",
    "\n",
    "plot_time = time_to_expiry_years[plot_indices]\n",
    "plot_moneyness = moneyness[plot_indices]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sns.histplot(moneyness, kde=False, ax=axes[0], bins=50)\n",
    "axes[0].set_title('')\n",
    "axes[0].set_xlabel('Moneyness $(S/K)$')\n",
    "axes[0].set_ylabel('Häufigkeit')\n",
    "axes[0].axvline(1.0, color='black', linestyle='--', label='At-the-Money (ATM)')\n",
    "axes[0].legend()\n",
    "\n",
    "sns.histplot(time_to_expiry_years, kde=False, ax=axes[1], bins=50)\n",
    "axes[1].set_title('')\n",
    "axes[1].set_xlabel('Restlaufzeit $t$ in Jahren')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=plot_time,\n",
    "    y=plot_moneyness,\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    edgecolor='none',\n",
    "    ax=axes[2],\n",
    "    rasterized=True\n",
    ")\n",
    "axes[2].axhline(1.0, color='black', linestyle='--', label='At-the-Money (ATM)')\n",
    "\n",
    "axes[2].set_title('')\n",
    "axes[2].set_xlabel('Restlaufzeit $t$ in Jahren')\n",
    "axes[2].set_ylabel('Moneyness $(S/K)$')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.savefig(\"put_colloc_sampling.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe24c54-3d05-40d1-ad54-6d79bd31f24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = Xc_pool[:, 0]\n",
    "K = Xc_pool[:, 1]\n",
    "t = Xc_pool[:, 2]\n",
    "\n",
    "moneyness = S / K\n",
    "\n",
    "time_to_expiry_years = (90 / 365.25) - t\n",
    "time_to_expiry_days = time_to_expiry_years\n",
    "\n",
    "n_total_points = Xc_pool.shape[0]\n",
    "n_plot_points = 10_000\n",
    "plot_indices = np.random.choice(n_total_points, n_plot_points, replace=False)\n",
    "\n",
    "plot_time = time_to_expiry_days[plot_indices]\n",
    "plot_moneyness = moneyness[plot_indices]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=plot_time,\n",
    "    y=plot_moneyness,\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    edgecolor='none'\n",
    ")\n",
    "\n",
    "plt.axhline(1.0, color='r', linestyle='--', label='At-the-Money (ATM)')\n",
    "\n",
    "plt.title('Scatter Plot of Collocation Points', fontsize=16)\n",
    "plt.xlabel('Days to Expiry', fontsize=12)\n",
    "plt.ylabel('Moneyness (S/K)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74472b92-3808-4014-8855-3fc52c2888c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S_ic = X_ic[:, 0]\n",
    "K_ic = X_ic[:, 1]\n",
    "moneyness_ic = S_ic / K_ic\n",
    "payoff_ic = Y_ic.flatten()\n",
    "\n",
    "n_total_points = X_ic.shape[0]\n",
    "n_plot_points = 10_000\n",
    "plot_indices = np.random.choice(n_total_points, n_plot_points, replace=False)\n",
    "\n",
    "plot_moneyness_ic = moneyness_ic[plot_indices]\n",
    "plot_payoff_ic = payoff_ic[plot_indices]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=plot_moneyness_ic,\n",
    "    y=plot_payoff_ic,\n",
    "    s=15,\n",
    "    alpha=0.6,\n",
    "    edgecolor='none'\n",
    ")\n",
    "\n",
    "plt.axvline(1.0, color='r', linestyle='--', label='At-the-Money (ATM)')\n",
    "\n",
    "plt.title('Scatter Plot of IC Points', fontsize=16)\n",
    "plt.xlabel('Moneyness (S/K)', fontsize=12)\n",
    "plt.ylabel('Option Payoff (Value at t=T)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74cba3a-0674-43cb-97ad-5044a6908e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "moneyness_bc = S_bc / K_bc\n",
    "payoff_bc = Y_bc.flatten()\n",
    "\n",
    "time_to_exp_years = (90 / 365.25) - X_bc[:, 2]\n",
    "\n",
    "mask_s0 = S_bc == 0\n",
    "mask_sMax = S_bc > 0\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=time_to_exp_years[mask_s0],\n",
    "    y=payoff_bc[mask_s0],\n",
    "    s=15,\n",
    "    alpha=0.7,\n",
    "    label=\"Boundary: S=0 (Value is always 0)\"\n",
    ")\n",
    "\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=time_to_exp_years[mask_sMax],\n",
    "    y=payoff_bc[mask_sMax],\n",
    "    s=15,\n",
    "    alpha=0.7,\n",
    "    label=\"Boundary: S>0 (Value decays with time)\"\n",
    ")\n",
    "\n",
    "plt.title(\"Boundary Conditions\", fontsize=16)\n",
    "plt.xlabel(\"Time to Expiry (Days)\", fontsize=12)\n",
    "plt.ylabel(\"Option Payoff\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
